# -*- coding: utf-8 -*-
"""dáil_app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pkriavd-S-YFOrOinJa2sSGhCzINVC8a
"""

import chromadb # Vector DB.
import json # storing examples for few-shot prompting.
import thefuzz[speedup] # for fuzzy look up of speaker name
import langchain # for few-shot prompting to provide examplar responses
import langchain_community
import openai # the model for generation
import streamlit as st # for app
import boto3, os # for loading vector db from AWS
from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate
from langchain.chat_models import ChatOpenAI

'''
1. Dáil Speeches
2. Vector DB
3. RAG
4. Question
'''
# 1. Dáil Speeches
  # See https://colab.research.google.com/drive/1pfbi0dFnQ8hae97J0LOnS3EhWfisQ5PI#scrollTo=Q-VDwJtzg_wi for more details as to how they are obtained.
  # Essentiall parsing XML files from URLs like, just varying the date: https://data.oireachtas.ie/akn/ie/debateRecord/dail/2013-05-03/debate/mul@/main.xml
# 2. Vector DB
  # Load vector DB from ChromaDB AWS
@st.cache_resource
def download_debate_db_from_s3(bucket_name, s3_prefix, local_dir):
    if not os.path.exists(local_dir):
        os.makedirs(local_dir, exist_ok=True)
    s3 = boto3.client('s3')
    for page in s3.get_paginator('list_objects_v2').paginate(Bucket=bucket_name, Prefix=s3_prefix):
        for obj in page.get('Contents', []):
            rel = os.path.relpath(obj['Key'], s3_prefix)
            dst = os.path.join(local_dir, rel)
            if not os.path.exists(dst):
                os.makedirs(os.path.dirname(dst), exist_ok=True)
                s3.download_file(bucket_name, obj['Key'], dst)
    return local_dir

bucket_name = 'joebucketai'
s3_prefix = 'debate_db/'
local_dir  = '/tmp/debate_db/'
debate_db_path = download_debate_db_from_s3(bucket_name, s3_prefix, local_dir)
# chroma client
chroma_client = chromadb.PersistentClient(
    path=debate_db_path
)
# get DB
collection = chroma_client.get_collection("oireachtas_debates")

# 3. RAG
# RETRIEVAL
# Function to get the k (default=5) most relevant quotes from vector DB
def search_speaker_position(speaker_name, topic, num_results=5):

    # Use ChromaDB's query functionality with `where` clause for speaker
    print("Looking for relevant utterences")
    results = collection.query(
        query_texts=[topic],
        n_results=num_results,
        where={"speaker": speaker_name},
        include=["metadatas"]
    )

    # Check if any results were found
    if not results['metadatas'][0]:  # ChromaDB returns a list of lists
        return f"No speeches found for {speaker_name}  talking about {topic} in the dataset."

    # Extract and format the results
    output = f"\n### {speaker_name}'s Position on '{topic}':\n"
    for i, metadata in enumerate(results['metadatas'][0]):
        output += f"\n **Quote {i+1} (debate url: {metadata['url']}):** {metadata['text'][:500]}...\n"

    return output

# FEW-SHOT PROMPTING
with open('./dail_examples.json', 'r', encoding='utf-8') as f:
    examples = json.load(f)

# formatting examples
example_prompt = ChatPromptTemplate.from_messages(
[('human', '{question}?'), ('ai', '{answer}\n')]
)

# combining examples with chat template
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
)

# combining system message with formatted examples, expecting user input {question}
full_prompt = ChatPromptTemplate.from_messages([
  ("system", "you are an Irish parliament chatbot, users will ask you questions about polititian's opinions on topics, you will provide summaries of their positions with reference to quotes that you will be provided as reference. You will not make anything up, you will not add quotes that are not relevant to the topic. These quotes have corresponding URLs, you will cite the quote using the URL and the year parsed from the URL as shown in the examples. You will value accuracy over plausibility."),
  few_shot_prompt,
  ("human", "{question}"),
])

# use nano for app so its cheaper
gpt_nano = ChatOpenAI(
    model_name="gpt-4.1-nano",
    temperature=0.9,
    openai_api_key=openai.api_key)

chain_nano = full_prompt | gpt_nano

# SPEAKERS

# read speakers.txt to get list of speakers
with open(project_fpath+"speakers.txt", "r") as f:
    SPEAKERS = f.read().splitlines()

# 4. Question
def speaker_fuzzy_lookup(speaker, speaker_list):
  # fuzzy lookup to get best match of speaker in list
  best_match = process.extractOne(speaker, speaker_list)
  return best_match[0]
'''
function that takes in the user's speaker and topic, finds the match in the speaker list.
Then generates a summary of the speaker on a topic given the debates.
'''
def generate_answer(speaker_name, topic, list_of_speakers, num_results=5):

    # the response to the user
    response = ""

    speaker_name = speaker_fuzzy_lookup(speaker_name, list_of_speakers)

    # Retrieve relevant quotes
    retrieved_text = search_speaker_position(speaker_name, topic, 5)

    if "No speeches found" in retrieved_text or "No relevant quotes found" in retrieved_text:
        return retrieved_text  # No results found, return directly

    response_nano = chain_nano.invoke({"question": f"Summarise {speaker_name}'s position on the topic: {topic}. Use the following quotes as reference: {retrieved_text}", "answer": ""})

    return response_nano.content

# get final answer
final_answer = generate_answer(speaker, topic,  SPEAKERS)